<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.549">

  <meta name="author" content="Gustavo Alckmin">
  <meta name="dcterms.date" content="2025-07-25">
  <title>AGRI4401 Lectures - L15 - Satellite System in Agriculture</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="AGRI4401_PrecisionAg.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="../site_libs/quarto-diagram/mermaid.min.js"></script>
  <script src="../site_libs/quarto-diagram/mermaid-init.js"></script>
  <link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">L15 - Satellite System in Agriculture</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Gustavo Alckmin 
</div>
</div>
</div>

  <p class="date">July 25, 2025</p>
</section>
<section id="satellite-data-for-precision-agriculture" class="slide level2">
<h2>Satellite Data for Precision Agriculture</h2>
<ul>
<li>Early precision farming relied on Landsat: 30 m spatial resolution, 15-day revisit, cloud interference<br>
</li>
<li>Spatial layers fed prescription maps for variable-rate fertilization, pesticide, and tillage<br>
</li>
<li>Baseline crop/soil status from surveys, field sampling, lab analysis plus point sensors<br>
</li>
<li>Critical need for frequent, cost-effective, fine-scale spatial data over heterogeneous fields<br>
</li>
<li>Emerging remote-sensing solutions: high-resolution satellites, hyperspectral, SAR
<ul>
<li>In WA: e.g., Sentinel-2 for reliable multispectral data, Sentinel-1 (SAR) for cloud penetration, and PlanetScope for high temporal resolution.</li>
</ul></li>
<li>Goal: real-time monitoring of crop nutrition, growth, yield, and soil properties
<ul>
<li>In WA: Tackling challenges like nutrient variability in sandy soils, and monitoring large-scale broadacre crops across the Wheatbelt.</li>
</ul></li>
</ul>
<aside class="notes">
<p>Precision agriculture’s evolution hinged on the ability to map field variability. In the 1990s, Landsat’s 30 m ground sample distance and 15-day overpass, further hampered by cloud cover, constrained temporal and spatial resolution. Nevertheless, these images enabled the first generation of prescription maps driving variable-rate fertilizer, pesticide, and tillage regimes tailored to management zones. Traditional agronomic assessment—surveys, soil cores, lab analyses—provided static snapshots, now supplemented by ground-truth data from local bodies like <strong>DPIRD</strong> and <strong>GRDC</strong> through trial sites and soil moisture probe networks.</p>
<p>The real technical barrier remains acquiring high-resolution, revisit-frequent spatial data across large acreages at acceptable cost. Advances in small-satellite constellations (PlanetScope, Sentinel series), UAV-mounted multispectral and hyperspectral sensors, and SAR platforms now offer sub-meter resolution and daily to weekly coverage. Integrating these data streams supports dynamic tracking of crop nutrition, phenology, water stress, biomass, and soil moisture, informing precise, in-season variable-rate applications. Research confirms that finer spatial detail and shorter revisit intervals can improve input use efficiency, yield consistency, and environmental outcomes by enabling truly site-specific management.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="agenda" class="slide level2">
<h2>Agenda</h2>
<ul>
<li>Introduction to Remote Sensing Principles for WA Agriculture</li>
<li>Key Satellite Platforms &amp; Data Sources for the WA Wheatbelt (Sentinel, Landsat, PlanetScope)</li>
<li>Application I: Mapping and Managing Soil</li>
<li>Application II: In-Season Crop Monitoring (NDVI, NDRE) for Wheat &amp; Canola</li>
<li>Application III: Integrating SAR for Soil Moisture Estimation</li>
<li>Data to Decision: Workflow from Satellite Imagery to VRT</li>
<li>Future Trends: AI, Data Fusion, and UAV integration in WA</li>
</ul>
<aside class="notes">
<p>This slide outlines the key topics we will cover. First, we will examine recent advancements in precision agriculture and integrated pest management tools, referencing pivotal studies that provide both modeling frameworks and field-validated applications. Next, we introduce the cognitive model of decision-making in IPM, based on March (1994), which frames how choices are structured. We then break down the three hierarchical levels of decisions: - Strategic: long-range planning at farm or landscape scale, including crop rotation and cultivar selection to pre-empt disease (Conway 1984; McCown 2002; Rossi et al.&nbsp;2012). - Tactical: short-term choices on when and which protection measures to use, optimizing interventions based on current crop and pest status. - Operational: real-time implementation and refinement of these tactics in response to evolving field conditions. Finally, we will transition into our detailed methodology and case studies to illustrate these concepts in action.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="what-is-remote-sensing" class="slide level2">
<h2>What is Remote Sensing?</h2>
<ul>
<li>Definition: Non-contact measurement of Earth features across visible and nonvisible EM spectra<br>
</li>
<li>Encompasses aerial, satellite, radar, and thermal instruments<br>
</li>
<li>Proximal sensing vs.&nbsp;remote sensing distinctions<br>
</li>
<li>Real-Time Kinematic (RTK) GPS for sub-cm geolocation<br>
</li>
<li>Rate controllers link spatial data to variable-rate applications</li>
<li>Applications in WA: e.g., mapping soil types in the Ord River Irrigation Area, monitoring canopy health (NDVI) in Margaret River vineyards, and delineating management zones in broadacre wheat fields.</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb1-1"><a></a>graph LR</span>
<span id="cb1-2"><a></a>  A[Remote Sensing] --&gt; B[Satellite Imagery]</span>
<span id="cb1-3"><a></a>  A --&gt; C[Aerial Photography]</span>
<span id="cb1-4"><a></a>  A --&gt; D[Radar]</span>
<span id="cb1-5"><a></a>  A --&gt; E[Thermal Imaging]</span>
<span id="cb1-6"><a></a>  A --&gt; F[Proximal Sensors]</span>
<span id="cb1-7"><a></a>  style A fill:#f9f,stroke:#333,stroke-width:1px</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>Remote sensing emerged in the late 1970s to describe Earth-observation beyond conventional aerial photography, extending into nonvisible wavelengths and spaceborne platforms. It refers to any measurement or information acquisition about a target without physical contact. This broad definition includes proximal sensors—devices placed near the target—and fully remote instruments on aircraft or satellites. Core technologies include color and color-infrared aerial photography, multispectral and hyperspectral satellite imagery, radar (e.g., SAR), and thermal imaging. RTK GPS enhances spatial accuracy by transmitting carrier-phase corrections from fixed reference stations to mobile units, a crucial component for <strong>controlled traffic farming (CTF)</strong> on large WA farms. In precision agriculture, remote sensing data feed rate controllers to apply inputs variably across fields. Key uses are early plant stress detection, mapping soil moisture and irrigation inefficiencies, distinguishing vegetation cover from bare soil, and defining management zones for site-specific treatments.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="electromagnetic-spectrum-in-agriculture" class="slide level2">
<h2>Electromagnetic Spectrum in Agriculture</h2>
<ul>
<li>Ultraviolet (100–400 nm) through microwave (1 mm–1 m) spectral range used in PA operations</li>
<li>Visible (400–700 nm) and NIR (700–1100 nm) bands for vegetation indices (e.g., NDVI, EVI)</li>
<li>SWIR (Short-Wave Infrared) bands are critical in WA for assessing soil and crop moisture content.</li>
<li>Multispectral vs.&nbsp;hyperspectral imaging: band count vs.&nbsp;spectral resolution trade-offs</li>
<li>Platforms like Sentinel-2 provide key bands (Visible, Red Edge, NIR, SWIR), while UAVs with sensors like MicaSense offer high-resolution data.</li>
<li>Non-imaging spectroradiometers capture point spectra across wide wavelength ranges</li>
<li>Thermal infrared (8–14 µm) sensors monitor canopy temperature and plant stress</li>
<li>Radar (3 MHz–110 GHz), LiDAR, ultrasonic and audio modalities for moisture, structure, topography and pest/equipment monitoring</li>
</ul>
<aside class="notes">
<p>Remote sensing in precision agriculture leverages the full electromagnetic spectrum to deliver actionable insights on crop health, water status, and field variability. Early PA systems in the 1980s relied on a few visible and near-infrared bands; by contrast, modern platforms span ultraviolet through microwave wavelengths, enhancing sensitivity to biochemical and physical crop parameters. Visible and NIR reflectance drive common vegetation indices such as NDVI and EVI, which correlate with chlorophyll content and biomass. Imaging sensors—multispectral (3–10 bands) versus hyperspectral (hundreds of narrow bands)—offer varying spectral resolutions for detecting subtle stress indicators. Non-imaging spectroradiometers provide high spectral fidelity at point locations, supporting calibration and ground-truthing. Thermal infrared instruments capture canopy temperature dynamics, an early stress indicator linked to transpiration and stomatal conductance. Beyond optical domains, radar backscatter reveals surface moisture and texture, LiDAR lasers map precise canopy structure and elevation, ultrasonic sensors determine leaf area index, and audio recordings detect pests or machinery operations. The integration of these modalities, along with necessary <strong>atmospheric correction</strong> for WA’s dusty conditions, facilitates comprehensive, site-specific crop management and maximizes resource use efficiency.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="blackbody-radiance" class="slide level2">
<h2>Blackbody Radiance</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-1-1.png" width="960" class="r-stretch"></section>
<section id="vegetation-spectra" class="slide level2">
<h2>Vegetation Spectra</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-2-1.png" width="960" class="r-stretch"></section>
<section id="sentinel-convolution" class="slide level2">
<h2>Sentinel Convolution</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-3-1.png" width="960" class="r-stretch"></section>
<section id="spectral-resolution-degradations" class="slide level2">
<h2>Spectral Resolution Degradations</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-4-1.png" width="960" class="r-stretch"></section>
<section id="spectral-regions" class="slide level2">
<h2>Spectral regions</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-5-1.png" width="960" class="r-stretch"></section>
<section id="passive-vs-active-remote-sensing" class="slide level2">
<h2>Passive vs Active Remote Sensing</h2>
<ul>
<li>Passive sensors rely on ambient solar radiation, capturing spectral reflectance across visible bands (blue, green, red)
<ul>
<li>In WA, performance is limited by winter cloud cover in the southwest and seasonal bushfire smoke.</li>
</ul></li>
<li>Require rigorous radiometric correction and precise image georectification to ensure quantitative accuracy</li>
<li>Performance constrained by clear-sky conditions, solar angle variability, and cloud cover</li>
<li>Active sensors emit their own illumination (e.g., LiDAR, RADAR), enabling data collection day/night and under cloudy skies
<ul>
<li>Sentinel-1 (SAR) is invaluable for mapping soil moisture and crop structure through clouds, crucial for in-season decisions in the grainbelt.</li>
</ul></li>
<li>Sensing range and spatial resolution limited by onboard light-source power, beam divergence, and platform payload capacity</li>
<li>Promising integration with UAS for rapid revisit rates, yet research is needed to extend operational range and reduce system weight</li>
</ul>
<aside class="notes">
<p>Passive remote sensing systems measure reflected solar energy across narrow wavelength bands to derive vegetation indices (like those using the <strong>red-edge</strong> for nitrogen status) and spectral signatures, but they depend on consistent solar illumination and clear skies. Accurate data acquisition requires incoming radiometric correction to normalize sensor response and precise georectification to align imagery with ground coordinates. Cloud cover and low sun angles introduce data gaps and variability, limiting timely decision-making. Active sensors, such as LiDAR and RADAR, overcome these limitations by emitting their own energy pulses, providing consistent returns regardless of ambient light or weather. However, their effective range and resolution are constrained by the power output and divergence of the onboard light source, posing engineering challenges for lightweight UAS deployment, which must adhere to <strong>CASA (Civil Aviation Safety Authority)</strong> regulations. Current research focuses on optimizing source efficiency, enhancing beam control, and reducing payload mass to enable extended-range active sensing for rapid, field-scale crop monitoring.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="spectra-by-lai" class="slide level2">
<h2>Spectra by LAI</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-6-1.png" width="960" class="r-stretch"></section>
<section id="radiances" class="slide level2">
<h2>Radiances</h2>

<img data-src="lecture_15_files/figure-revealjs/unnamed-chunk-7-1.png" width="960" class="r-stretch"></section>
<section id="key-components-of-satellite-imaging-systems" class="slide level2">
<h2>Key Components of Satellite Imaging Systems</h2>
<ul>
<li>Cost-effective monitoring of crop and soil variability across paddock to regional scales</li>
<li>Spatial resolution: pixel ground size defining the smallest detectable feature</li>
<li>Spectral resolution: number and width of bands for discriminating vegetation and soil signatures</li>
<li>Radiometric resolution: digital levels per band for detecting subtle reflectance differences</li>
<li>Temporal resolution: revisit frequency critical for tracking growth dynamics and stress events</li>
<li>Sensor selection depends on the WA application: Sentinel-2 for paddock-scale variability, PlanetScope for daily monitoring of high-value crops, and Landsat for long-term historical analysis.</li>
</ul>
<aside class="notes">
<p>Optical remote sensing has become a cornerstone in precision agriculture by providing timely, spatially explicit data on crop health and soil properties. Spatial resolution determines the smallest object we can detect – sub-meter pixels capture individual plants, while coarser resolutions are suited for field-scale variability. Spectral resolution dictates the number and bandwidths of captured wavelengths: multispectral sensors (e.g., Landsat) versus hyperspectral systems offer trade-offs in band quantity, bandwidth, and data volume. Radiometric resolution affects the sensor’s sensitivity to reflectance differences; higher bit depths enable detection of subtle vegetation stress or soil moisture changes. Temporal resolution, or revisit interval, underpins our ability to observe phenological stages and stress events. For example, while daily PlanetScope data is powerful, the cost and high probability of cloud cover during the winter growing season must be considered, often making the free, 3-4 day revisit of Sentinel-2 a more practical choice for broadacre applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="practical-orbital-characteristics" class="slide level2">
<h2>Practical Orbital Characteristics</h2>
<ul>
<li><strong>Sun-Synchronous Orbit:</strong> Most Earth observation satellites (like Landsat and Sentinel) are in a sun-synchronous orbit. This means they pass over any given point on Earth at the same local solar time, which provides consistent illumination for comparing images over time.</li>
<li><strong>Revisit Frequency vs.&nbsp;Swath Width:</strong> There is a trade-off between how often a satellite can image a location (revisit time) and how wide an area it can see (swath width). Wider swaths allow for more frequent revisits.</li>
<li><strong>Satellite Constellations:</strong> Companies like Planet operate large constellations of small satellites (above 240 active). This allows them to achieve very high temporal resolution (e.g., daily revisits) by having many satellites working together.</li>
</ul>
<aside class="notes">
<p>Understanding a satellite’s orbit is key to using its data effectively. A sun-synchronous orbit is crucial for agricultural monitoring because it minimizes variations in shadows and illumination, making it easier to detect actual changes in crop health over time rather than changes caused by the sun’s angle. The concept of revisit time is also critical; a 5-day revisit means you get a new image every 5 days, which is vital for tracking rapid growth stages or stress events. However, this is often a trade-off with the level of detail the satellite can capture. Finally, the rise of satellite constellations has been a game-changer, offering the kind of high-frequency monitoring that was previously only possible with expensive aerial surveys, though cloud cover remains a practical limitation.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="resolutions-in-satellite-imagery" class="slide level2">
<h2>Resolutions in Satellite Imagery</h2>
<ul>
<li>Spatial resolution: defines ground sample distance (e.g., 0.3–500 m)</li>
<li>Spectral resolution: number and width of wavelength bands (e.g., multispectral, hyperspectral)</li>
<li>Radiometric resolution: quantization levels per band (e.g., 8–12 bits)</li>
<li>Temporal resolution: revisit frequency (e.g., daily to biweekly)</li>
<li>Sensor examples for WA: PlanetScope (~3-5 m), Sentinel-2 (10-20 m), Landsat 8/9 (30 m), and MODIS (250-500 m for regional views).</li>
<li>Positional Accuracy: Expect ~5-10 m geolocation error for free data (Sentinel/Landsat), suitable for paddock zones but may require ground control for precision alignment.</li>
<li>Application impacts: crop stress detection, yield forecasting, input zoning</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb2-1"><a></a>graph LR</span>
<span id="cb2-2"><a></a>  A[Satellite Imagery Resolutions] --&gt; B[Spatial Resolution]</span>
<span id="cb2-3"><a></a>  A --&gt; C[Spectral Resolution]</span>
<span id="cb2-4"><a></a>  A --&gt; D[Radiometric Resolution]</span>
<span id="cb2-5"><a></a>  A --&gt; E[Temporal Resolution]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>In precision agriculture, selecting the right satellite sensor hinges on understanding four core resolution dimensions. Spatial resolution determines the ground area each pixel represents: sub-meter systems enable plant-level diagnostics, whereas coarser sensors suit regional analyses. Spectral resolution—the number and bandwidth of spectral channels—drives the ability to discern crop biochemical and structural properties. Radiometric resolution influences sensitivity to subtle reflectance changes critical for early stress detection. Temporal resolution, defined by revisit frequency, affects the timeliness of management decisions. <strong>Note: The revisit frequency of polar-orbiting satellites like Sentinel-2 improves at higher latitudes, resulting in a ~3-4 day revisit over WA, which is better than the global average.</strong> When generating interpolated rasters, the effective resolution mirrors input sample density. Balancing these attributes against cost, data volume, and agronomic objectives is essential for optimized field-scale monitoring.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="overview-of-satellite-data-sources" class="slide level2">
<h2>Overview of Satellite Data Sources</h2>
<ul>
<li>Optical imagery (e.g., Sentinel-2) provides key vegetation indices like NDVI (using 10m Red and NIR bands) and NDRE (using Red Edge bands) for crop health.</li>
<li>Synthetic Aperture Radar (e.g., Sentinel-1 C-band) provides reliable soil moisture and crop structure data, unaffected by cloud cover.</li>
<li>Historical archives (Landsat back to the 1980s) to validate and refine multi-year management zones</li>
<li>High revisit frequencies (Sentinel-2: ~3-4 d; PlanetScope: daily) enabling near-real-time crop monitoring</li>
<li>Data fusion frameworks combining optical and radar to improve spatial and temporal coverage</li>
<li>Collaborative analytics pipelines from <strong>DPIRD, CSIRO, and GRDC</strong> are key for developing and validating WA-specific models for management zone (MZ) delivery.</li>
</ul>
<aside class="notes">
<p>Satellite platforms provide the foundational data streams necessary for precise management zone (MZ) delineation. Optical sensors like Sentinel-2 capture multispectral reflectance at 10–30 m, generating key vegetation indices (e.g., NDVI, EVI) that track crop health dynamics. Radar sensors (e.g., Sentinel-1 SAR) penetrate clouds and offer direct estimates of soil moisture and structural parameters, critical in regions with frequent overcast. Historical archives—spanning Landsat missions since the 1980s—enable retrospective analysis to assess MZ stability and adapt management strategies across seasons. Increasing revisit rates from constellations like PlanetScope (up to daily) facilitate near-real-time anomaly detection. Advanced data fusion algorithms integrate optical and radar streams to overcome individual sensor limitations, yielding more reliable spatial maps. Achieving rapid, accurate MZs at scale relies on synergistic collaboration between academic research teams and private agritech providers, co-developing big-data pipelines and machine learning models tailored for field-scale actionable insights.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="intro-to-planet-sentinel-and-landsat" class="slide level2">
<h2>Intro to Planet, Sentinel, and Landsat</h2>
<ul>
<li><strong>Landsat 8/9:</strong> Multispectral (30 m), Thermal IR (100 m native, resampled to 30 m); 8-day combined revisit.</li>
<li><strong>Sentinel-2:</strong> 10 m &amp; 20 m MSI including multiple Red Edge &amp; SWIR bands; ~3-4 day revisit over WA.</li>
<li><strong>PlanetScope:</strong> ~3-5 m resolution with daily revisit potential, though limited by cloud cover during the winter growing season.</li>
<li><strong>WorldView-2:</strong> Panchromatic &amp; 8-band MSI (Coastal, Yellow, Red Edge, NIR); 0.5–1.8 m; daily revisit (High-cost commercial).</li>
<li><strong>Historical Milestones:</strong> Landsat-1 launch (1972); LACIE multi-date yield modeling (1974–75)</li>
</ul>
<aside class="notes">
<p>This slide provides a survey of satellite sensors central to precision agriculture. Landsat 8/9, with its moderate spatial resolutions and thermal band, supports long-term analysis and evapotranspiration monitoring. Sentinel-2’s frequent revisit and red-edge/SWIR capabilities enable detailed vegetation analysis and are a workhorse for WA agriculture. PlanetScope offers near-daily, high-resolution data ideal for rapid field scouting or high-value crops, but its utility can be limited by cost and cloud cover. WorldView-2 delivers very high-resolution data, suitable for research or specialized applications. The choice of sensor is a trade-off. For example, during the critical winter cropping season, the high probability of cloud means that out of 30 potential PlanetScope images in a month, only a few might be usable. The consistent revisit of Sentinel-1 (SAR) or the higher chance of a cloud-free image from the 3-4 day Sentinel-2 cycle is often more practical. Historically, the 1972 Landsat-1 launch pioneered routine Earth observation, and the 1974–75 LACIE program demonstrated multi-date spectral signature modeling for wheat yield estimation, establishing methodologies foundational to today’s remote sensing in agriculture.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="spatial-resolution-comparison" class="slide level2">
<h2>Spatial Resolution Comparison</h2>
<ul>
<li>SPOT 5: 10 m × 10 m, MODIS: 500 m × 500 m, Aerial: ~0.5 m × 0.5 m per pixel<br>
</li>
<li>High resolution (&lt; 1 m) reveals fine-scale features; low resolution (&gt; 30 m) aggregates data over larger areas<br>
</li>
<li>GIS-derived raster cell size dictates analysis granularity (1 m vs.&nbsp;30 m grid; 30 m pixel = 900 × 1 m cells)<br>
</li>
<li>Trade-offs: detail vs.&nbsp;data volume, acquisition cost, and processing requirements<br>
</li>
<li>Operational alignment: match resolution to implement scale (e.g., 90 m machinery swaths)<br>
</li>
<li>Balance spatial resolution with temporal frequency for dynamic field monitoring</li>
</ul>
<aside class="notes">
<p>Spatial resolution defines the ground area each pixel covers and is a fundamental parameter in precision agriculture data analysis. Satellite sensors like SPOT 5 provide 10 m resolution, suitable for regional crop vigor mapping, whereas MODIS at 500 m captures large-scale patterns but misses field-level variability. Aerial platforms can achieve sub-meter resolution (~0.5 m), allowing detection of individual plants or irrigation anomalies. In raster-based GIS products, the grid cell size (e.g., 1 m vs.&nbsp;30 m) determines the minimum feature size detectable; a 30 m cell aggregates 900 one-meter cells, smoothing fine details. However, higher resolution yields exponentially larger datasets, increasing storage, transmission, and processing costs. Practical selection of resolution must consider sensor/platform availability, cost constraints, and the scale of agricultural operations—using a resolution finer than farm equipment swath width (e.g., 90 m) may yield redundant data. Finally, spatial resolution choices should be integrated with temporal resolution planning to capture dynamic processes like pest outbreaks or irrigation events at appropriate intervals.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="temporal-resolution-comparison" class="slide level2">
<h2>Temporal Resolution Comparison</h2>
<ul>
<li>Short intervals yield high temporal resolution (e.g., weekly field scouting vs.&nbsp;monthly visits)<br>
</li>
<li>Satellite revisit cycles: daily (24 h) for high vs.&nbsp;16-day intervals for low<br>
</li>
<li>High-frequency data essential for statistical process control to detect anomalies<br>
</li>
<li>Thematic detail vs.&nbsp;revisit frequency trade-off impacts attribute resolution<br>
</li>
<li>Inherent spatial–temporal resolution trade-offs constrain monitoring strategies<br>
</li>
<li>Integrating multi-scale datasets is critical for accurate crop stress detection</li>
</ul>
<aside class="notes">
<p>Temporal resolution refers to how often observations are made. In precision agriculture, weekly or even daily data capture allows for timely detection of deviations in growth or stress. Satellite platforms demonstrate this: a 24-hour revisit provides daily imagery, while sensors with a 16-day cycle offer much coarser temporal sampling. Research in statistical process control (SPC) shows that high-frequency measurements significantly improve the detection of outliers and abnormal crop behaviors. However, increasing thematic detail (e.g., adding soil type or crop variety attributes) often reduces achievable revisit rates due to sensor and processing constraints. This trade-off between spatial, temporal, and thematic resolutions is a core challenge: fusing datasets collected at different scales requires careful alignment of timestamps and rigorous resampling methods. Proper resolution selection and data fusion are vital for actionable insights in crop monitoring and stress detection.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="spectral-bands-comparison" class="slide level2">
<h2>Spectral Bands Comparison</h2>
<ul>
<li>NDVI exploits contrast between Red (0.63–0.69 µm) and NIR (0.76–0.90 µm) reflectance<br>
</li>
<li>Spectral signature: unique reflectance patterns across bands for material identification<br>
</li>
<li>Spectral resolution defines minimum wavelength separation for distinguishing fine features<br>
</li>
<li>Spectral response characterizes sensor sensitivity and bandpass characteristics<br>
</li>
<li>Band selection influences detection of chlorophyll absorption, biomass, and water stress<br>
</li>
<li>Spectroradiometer measurements provide high-fidelity data for calibration and validation</li>
</ul>
<aside class="notes">
<p>This slide delves into the core concepts behind comparing spectral bands for precision agriculture. First, we highlight how NDVI—one of the most prevalent vegetation indices—relies on the strong contrast between red and near-infrared reflectance to quantify chlorophyll activity and canopy structure. Modern sensors such as Landsat 8 OLI capture red at 0.64 µm and NIR at 0.85 µm, enabling continuous monitoring of plant health.</p>
<p>Next, we define the spectral signature as the characteristic pattern of reflectance or emittance across multiple discrete bands. Each material—whether leaf pigments, soil minerals, or crop residues—exhibits a distinct signature, akin to molecular fingerprints in nuclear magnetic resonance spectroscopy. High spectral resolution (e.g., 5 nm vs.&nbsp;10 nm band spacing) enhances our ability to isolate subtle absorption features attributable to trace elements or water content.</p>
<p>The spectral response function of a sensor describes its sensitivity curve within a given bandpass, influencing both signal-to-noise ratio and calibration accuracy. Understanding these response functions is critical when comparing data from different platforms or designing custom indices for specific biophysical parameters such as biomass, leaf area index, or moisture stress.</p>
<p>Finally, ground-truth measurements from a field spectroradiometer provide detailed spectral curves across 350–2500 nm. These high-fidelity data sets are used to calibrate satellite or UAV-based sensors, validate remote estimates, and refine band selection strategies to optimize detection of water stress, nutrient deficiencies, and crop phenology stages.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-access-and-licensing" class="slide level2">
<h2>Data Access and Licensing</h2>
<ul>
<li>Distinguish data ownership vs.&nbsp;access rights in service-level agreements<br>
</li>
<li>Evaluate licensing models (perpetual, subscription, data-as-a-service)<br>
</li>
<li>Integrate centralized on-farm database via OEM and contractor partnerships<br>
</li>
<li>Secure data transfer from onboard devices using standardized APIs (ISOXML, ADAPT)<br>
</li>
<li>Address legal and ethical considerations: usage rights and unintended interpretations<br>
</li>
<li>Recognize barriers to cloud adoption: ownership ambiguity, financial data sensitivity</li>
</ul>
<aside class="notes">
<p>Precision agriculture systems increasingly rely on robust data governance frameworks that delineate ownership and access rights. Research (Smith et al., 2020) indicates that clear contractual definitions between producers and service providers reduce legal disputes and align expectations.</p>
<p>Licensing models in precision agriculture range from perpetual licenses to subscriptions and data-as-a-service offerings. Each structure presents distinct cost profiles and usage rights, as detailed by Johnson and Miller (2019).</p>
<p>Modern PA implementations integrate on-board sensor and machine data with centralized farm databases through OEM and third-party contractor partnerships. Standardized APIs such as ISOXML and ADAPT enable secure, interoperable data exchanges (ISO, 2018).</p>
<p>Legal and ethical frameworks must address unintended interpretations of analytics outputs and ensure compliance with regulations like GDPR and CCPA. This is especially critical when financial and operational datasets can reveal competitive insights.</p>
<p>Despite the technical maturity of cloud storage solutions, adoption remains limited due to unresolved ownership ambiguities, concerns over vendor lock-in, and the sensitivity of farm financial information (Nguyen et al., 2021).</p>
<p>Next: Data Privacy and Compliance</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-volume-and-processing-requirements" class="slide level2">
<h2>Data Volume and Processing Requirements</h2>
<ul>
<li>On-farm storage: proprietary local databases for fertilizer, protection, and yield data; full data ownership but limited capacity<br>
</li>
<li>Remote (“cloud-ready”) solutions: manufacturer/third-party servers enabling predictive maintenance and aggregated performance insights<br>
</li>
<li>Web/cloud services: highly scalable storage and on-demand applications, rapid feature updates; underutilized due to security and ownership concerns<br>
</li>
<li>Data volume: multi-terabyte seasonal streams from sensors, GPS, yield monitors, and UAV imagery<br>
</li>
<li>Processing requirements: high-throughput data ingestion, edge-to-cloud synchronization, real-time analytics, and machine learning pipelines<br>
</li>
<li>Integration workflows: emerging standardized protocols and APIs, but trust, privacy, and provider lock-in remain barriers</li>
</ul>
<aside class="notes">
<p>Agronomic data workflows are generating multi-terabyte datasets each season from an array of in-field sensors, yield monitors, and UAV imaging. On-farm storage solutions use local databases and proprietary software, delivering tailored analyses and full data ownership but constrained by hardware limits and manual maintenance. Remote or “cloud-ready” systems shift processing to manufacturer or third-party servers, offering predictive maintenance, automated updates, and aggregated machine performance insights, yet raise concerns around data ownership, security, and provider dependency. Fully web-based cloud services enable virtually unlimited storage, on-demand access to specialized applications, and rapid feature rollouts; however, adoption is stifled by unclear data ownership policies, privacy regulations, and financial confidentiality issues. High-throughput ingestion frameworks and real-time analytics are critical to extract actionable insights, while emerging standards for API-driven data exchange promise smoother integration—contingent on building trust and ensuring robust data governance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="atmospheric-correction" class="slide level2">
<h2>Atmospheric Correction</h2>
<ul>
<li>Use MODTRAN-based models to estimate atmospheric transmittance in VIS-NIR bands<br>
</li>
<li>Correct top-of-atmosphere radiance to surface reflectance employing the 6S radiative transfer model<br>
</li>
<li>Incorporate aerosol optical depth data from AERONET or satellite-derived sources for aerosol scattering corrections<br>
</li>
<li>Apply water vapor and ozone absorption adjustments using temporally aligned meteorological profiles<br>
</li>
<li>Include adjacency effect correction to mitigate reflectance contamination from neighboring pixels<br>
</li>
<li>Validate corrected reflectance against in-situ spectral ground-truth measurements to maintain &lt; 5 % RMSE</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb3-1"><a></a>flowchart LR</span>
<span id="cb3-2"><a></a>  A[Raw Top-of-Atmosphere Radiance] --&gt; B[Sensor Radiometric Calibration]</span>
<span id="cb3-3"><a></a>  B --&gt; C[6S Atmospheric Model]</span>
<span id="cb3-4"><a></a>  C --&gt; D[Apply Atmospheric Correction]</span>
<span id="cb3-5"><a></a>  D --&gt; E[Derive Surface Reflectance]</span>
<span id="cb3-6"><a></a>  E --&gt; F[Validation with In-situ Spectra]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>Atmospheric correction is critical for transforming top-of-atmosphere measurements into accurate surface reflectance values used in precision agriculture. We use MODTRAN-based radiative transfer models, notably the Second Simulation of the Satellite Signal in the Solar Spectrum (6S) model (Vermote et al., 1997; Kotchenova &amp; Vermote, 2007). The process begins with radiometric calibration to convert digital numbers to spectral radiance, using pre-flight and vicarious calibration coefficients. Aerosol optical depth is derived from AERONET observations or satellite retrievals, providing key inputs for aerosol scattering corrections. Water vapor and ozone profiles are obtained from near-coincident meteorological soundings or reanalysis datasets to correct gaseous absorption features. The adjacency effect, caused by mixed-pixel contributions, is corrected using a spatial smoothing kernel to ensure pixel-specific purity. Finally, we validate the surface reflectance products against field spectroradiometer measurements, targeting an RMSE below 5 % as per Li et al.&nbsp;(2018). Implementations are integrated into QGIS workflows using the ORFEO Toolbox and Python scripts for scalability across mosaic tiles.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="radiometric-calibration" class="slide level2">
<h2>Radiometric Calibration</h2>
<ul>
<li>Reflectance(λ) = [DN(λ) – DN_dark(λ)] / [DN_white(λ) – DN_dark(λ)]<br>
</li>
<li>DN(λ): raw digital-number response per pixel at wavelength λ<br>
</li>
<li>DN_dark(λ): dark current response (sensor bias correction)<br>
</li>
<li>DN_white(λ): white reference plate response (normalization factor)<br>
</li>
<li>Subtract DN_dark to remove sensor bias and fixed-pattern noise<br>
</li>
<li>Divide by dark-corrected white signal to obtain true reflectance</li>
</ul>
<aside class="notes">
<p>In this slide, we detail the radiometric calibration workflow essential for camera-based canopy reflectance measurements. Raw digital numbers (DN) include both scene radiance and sensor bias (dark current). By capturing a dark frame (completely shuttered or covered sensor), we measure DN_dark, which accounts for electronic noise and per-pixel offsets. A calibrated white reference plate provides DN_white, representing the sensor’s response to a known reflectance standard (ideally 100 %). Subtracting DN_dark from both raw and white measurements corrects for sensor bias. Dividing the bias-corrected DN by the bias-corrected white reference normalizes the output to unitless reflectance values. This procedure mitigates temperature drift, non-uniform pixel response, and illumination variability. Consistent white-plate calibration and frequent dark-frame collection are critical for high-precision vegetation index computation and temporal comparability. Next, we will examine geolocation accuracy and the impact of differential GNSS correction on spatial precision.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="geometric-correction-orthorectification" class="slide level2">
<h2>Geometric Correction &amp; Orthorectification</h2>
<ul>
<li>Uses latitude &amp; longitude on a spheroid to establish GCS<br>
</li>
<li>Applies ground control points (GCPs) for geometric correction<br>
</li>
<li>Considers GDOP to minimize GPS positional errors<br>
</li>
<li>Assigns geographic coordinates via georeferencing<br>
</li>
<li>Incorporates DEMs to orthorectify terrain-induced displacement<br>
</li>
<li>Refines map topology &amp; clipping for accurate layer overlays</li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb4-1"><a></a>flowchart LR</span>
<span id="cb4-2"><a></a>  RAW[Raw Remote Sensing Image] --&gt; GCP[Ground Control Points]</span>
<span id="cb4-3"><a></a>  GCP --&gt; GeoCorr[Geometric Correction]</span>
<span id="cb4-4"><a></a>  GeoCorr --&gt; Orth[Orthorectification]</span>
<span id="cb4-5"><a></a>  Orth --&gt; OrthoImage[Ortho-rectified Image]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>In precision agriculture, ensuring spatial accuracy of remote sensing imagery is foundational for reliable field analysis and decision-making. Geometric correction aligns images to known geographic coordinate systems by matching image pixels to ground control points, compensating for sensor distortions and platform motion. The selection of GCPs must account for Geometric Dilution of Precision (GDOP) when using GPS-derived coordinates, as suboptimal satellite configurations can degrade positional accuracy. Following geometric correction, orthorectification uses digital elevation models to remove terrain-induced distortions, producing planimetrically accurate orthoimages suitable for multi-temporal comparisons and GIS integration. Accurate orthoimages enable precise overlay of soil, crop yield, and management zone layers, critical for variable rate applications and in-depth spatial analyses.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="cloud-detection-masking" class="slide level2">
<h2>Cloud Detection &amp; Masking</h2>
<ul>
<li>Cloud cover obstructs optical remote sensing during key crop growth stages<br>
</li>
<li>Radar sensors (e.g., SAR) and UAS imaging penetrate or bypass clouds for continuous data acquisition<br>
</li>
<li>Spatial resolution vs.&nbsp;coverage trade-off: high-resolution imagery captures fine detail over limited areas; low-resolution rasters cover broad regions<br>
</li>
<li>Spectral resolution ranges from RGB to multispectral and hyperspectral sensors for advanced vegetation analysis<br>
</li>
<li>Cloud masking algorithms (e.g., Fmask, QA bands) automatically detect and exclude cloudy pixels from imagery<br>
</li>
<li>Cloud-free composites integrate masked data streams to provide uninterrupted temporal monitoring</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb5-1"><a></a>flowchart LR</span>
<span id="cb5-2"><a></a>  A[Satellite Optical Data] --&gt; B{Cloud Cover?}</span>
<span id="cb5-3"><a></a>  B -- Yes --&gt; C[Radar &amp; UAS Acquisition]</span>
<span id="cb5-4"><a></a>  B -- No --&gt; D[Process Optical Imagery]</span>
<span id="cb5-5"><a></a>  C --&gt; D</span>
<span id="cb5-6"><a></a>  D --&gt; E[Cloud Masking (Fmask/QA Bands)]</span>
<span id="cb5-7"><a></a>  E --&gt; F[Cloud-Free Composite Generation]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>Reliable remote sensing under variable weather is critical for precision agriculture. Optical sensors deliver high spatial and spectral detail but fail under cloud cover. Synthetic aperture radar (SAR) is weather-independent, penetrating clouds to collect consistent backscatter data for soil moisture and structure analysis. Unmanned aerial systems (UAS) operate below the cloud layer, acquiring ultra-high-resolution imagery that complements satellite data. Users must balance spatial resolution and coverage: fine-scale mapping reveals within-field variability, while coarse-resolution rasters monitor regional trends. Spectral resolution extends from RGB imaging to multispectral (e.g., red-edge, NIR) and hyperspectral sensors, enabling computation of vegetation indices like NDVI and PRI. Cloud masking techniques, such as the Fmask algorithm and quality assessment (QA) bands embedded in products like Landsat and Sentinel, identify cloudy and shadowed pixels using spectral thresholds and thermal data. Removing these pixels and mosaicking clear observations generates cloud-free composite maps, which underpin accurate monitoring of crop health, phenology, and stress over the growing season.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="challenges-in-using-satellite-imagery" class="slide level2">
<h2>Challenges in Using Satellite Imagery</h2>
<ul>
<li>Cloud cover and atmospheric scattering degrade spectral fidelity<br>
</li>
<li>Spatial resolution limits detection of sub-field variability<br>
</li>
<li>Limited spectral bands hinder discrimination of crop stress signatures<br>
</li>
<li>Radiometric calibration drift reduces temporal comparability<br>
</li>
<li>Infrequent revisit intervals miss rapid phenological changes<br>
</li>
<li>High data volumes demand robust processing and storage infrastructure</li>
</ul>
<aside class="notes">
<p>This slide outlines six primary technical challenges when applying satellite imagery in precision agriculture. Cloud cover and atmospheric effects can obscure or alter surface reflectance, requiring complex correction and masking algorithms that reduce the amount of usable data. Spatial resolution trade-offs—such as Sentinel-2’s 10 m versus MODIS’s 250 m pixel size—limit the detection of fine-scale variability within fields, making it harder to target management zones. The limited number of spectral bands in multispectral sensors restricts the ability to distinguish subtle physiological stress indicators that hyperspectral instruments could reveal. Radiometric calibration drift across sensors and over time introduces inconsistencies, demanding rigorous cross-calibration and normalization workflows to compare imagery across dates. Temporal resolution constraints, with typical revisit intervals of 5–16 days, may miss transient events such as pest outbreaks or short-lived moisture stress. Finally, the high volume of high-resolution imagery generates large datasets that require substantial computational and storage infrastructure, often necessitating cloud-based processing or high-performance local systems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="detecting-crop-health-with-imagery" class="slide level2">
<h2>Detecting Crop Health with Imagery</h2>
<ul>
<li>Platforms: UAVs, manned aircraft, and satellites offering complementary spatial and temporal coverage<br>
</li>
<li>Sensors: High-resolution multispectral and hyperspectral imagers capturing narrow bands tied to pigments, moisture, and structural traits<br>
</li>
<li>Spectral Signatures: Unique reflectance patterns reveal nutrient deficiencies, water stress, and disease onset<br>
</li>
<li>Multi-Temporal Analysis: Sequential image acquisitions enable detection of symptom progression and early warning<br>
</li>
<li>Stress Discrimination: Fusion of spectral time series with soil and crop data differentiates overlapping stresses and disease complexes<br>
</li>
<li>Industry Trend: Declining sensor costs and democratization of data analytics accelerate adoption in precision agriculture</li>
</ul>
<aside class="notes">
<p>This slide examines the state-of-the-art remote sensing tools used in site-specific crop health monitoring. We begin with platforms: UAVs offer centimeter-level detail and rapid deployment, manned aircraft cover larger areas on demand, and satellites provide routine, broad-scale observations. Next, sensors range from 5–10 spectral bands in multispectral systems to hundreds of contiguous bands in hyperspectral instruments. These sensors capture key wavelengths where chlorophyll, carotenoids, water, and cell structure interact with light. By analyzing spectral signatures, we can identify single dominant issues such as nitrogen deficiency or leaf blight. However, when multiple stressors co-occur, we rely on multi-temporal imagery and dynamic disease models—integrating soil moisture and crop phenology data—to disentangle causes. Repeated acquisitions help track the spatiotemporal progression of symptoms, improving early-warning systems and enabling timely interventions. Finally, note the industry trend: as sensor and processing costs fall, more growers and researchers are embedding fine-resolution imagery into their precision-ag tools, boosting accuracy and reducing decision latency.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="disease-and-pest-outbreak-detection" class="slide level2">
<h2>Disease and Pest Outbreak Detection</h2>
<ul>
<li>Decision support tools (warning services, DSS) vary in spatial/temporal resolution and data sources<br>
</li>
<li>On-site sensing: advanced computer vision, spectral imaging, and machine-learning techniques<br>
</li>
<li>Communication channels: SMS alerts, web portals, and mobile applications<br>
</li>
<li>Delivery modes: real-time notifications, periodic reports, and interactive dashboards<br>
</li>
<li>Current best practice: integrated visual scouting with instrumental sensors<br>
</li>
<li>Population-dynamics model integration for precise treatment timing and dosage</li>
</ul>
<aside class="notes">
<p>Early detection of plant disease and pest outbreaks is critical for minimizing yield losses. A variety of decision support tools have been developed: warning services and full decision support systems (DSS) that differ in their spatial and temporal resolution, data provenance (public vs.&nbsp;private), and delivery mechanisms. On-site sensing technologies now leverage advanced computer vision algorithms, hyperspectral and multispectral imaging, and machine-learning classifiers to detect subtle pathogen-induced changes before symptoms are visible.</p>
<p>Growers receive these insights through multiple channels, from SMS alerts to web-based dashboards and mobile apps, allowing for rapid, informed action. While fully automated, real-time disease detection remains an active research frontier, current best practice combines manual visual scouting with instrumented measurements to confirm pathogen presence at its onset. Integrating field observations with population-dynamics models enables optimized intervention decisions—specifically timing and dosage of treatments—reducing chemical inputs and improving sustainability. A seminal review by Sankaran et al.&nbsp;(2010) highlights the potential of these emerging technologies to shift precision crop protection from reactive to proactive disease management.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="inferring-soil-moisture-from-spectral-data" class="slide level2">
<h2>Inferring Soil Moisture from Spectral Data</h2>
<ul>
<li>Hyperspectral imaging systems capture water absorption features at narrow bands (e.g., 1.4 μm, 1.9 μm)<br>
</li>
<li>Multivariate regression models (PLSR, SVR, Random Forest) applied to reflectance spectra<br>
</li>
<li>RMSE improvements of 10–20 % over multispectral NDWI-based moisture indices<br>
</li>
<li>Calibration using in-situ dielectric soil moisture probes and k-fold cross-validation<br>
</li>
<li>Sensitivity to confounders: surface roughness, organic matter content, sensor noise<br>
</li>
<li>Local-scale accuracy high; challenges remain for large-area, multisite generalization</li>
</ul>
<aside class="notes">
<p>This slide reviews the state of the art in deriving soil moisture from remote sensing. Hyperspectral sensors, with dozens to hundreds of narrow contiguous bands, resolve subtle water absorption features around 1.4 μm and 1.9 μm that are not accessible to traditional multispectral platforms like Landsat or MODIS. By feeding these high-dimensional spectra into multivariate regression algorithms—such as Partial Least Squares Regression (PLSR), Support Vector Regression (SVR), and Random Forest—researchers achieve root-mean-square error (RMSE) improvements of 10–20 % compared to standard NDWI or spectral index approaches. Calibration typically relies on in-situ probes for ground truth and employs rigorous k-fold cross-validation to guard against overfitting. Despite these advances, spectral signal confounders (e.g., surface roughness, variable organic matter, sensor noise) and the lack of unified large-area protocols limit scalability. Reviews by Yufeng et al.&nbsp;(2011) and Mulder et al.&nbsp;(2011) catalog these methods and underscore the need for integrated frameworks to extend local successes to regional or continental scales.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="limitations-and-integrations" class="slide level2">
<h2>Limitations and Integrations</h2>
<ul>
<li>Regulatory and privacy constraints limit spatial data sharing<br>
</li>
<li>Proprietary formats create siloed agronomic datasets<br>
</li>
<li>Absence of interoperability protocols hinders cross-platform integration<br>
</li>
<li>ADAPT API: open-source toolkit for seamless data translation<br>
</li>
<li>Modular plug-ins support diverse agricultural standards<br>
</li>
<li>Facilitates unified workflows and accelerates technology adoption</li>
</ul>
<aside class="notes">
<p>Precision agriculture relies heavily on proximal sensing and GIS systems to capture high-resolution soil and crop data. However, legal and regulatory barriers around data privacy often restrict the free exchange of spatial information, creating fragmented data silos. Additionally, the proliferation of proprietary formats and the absence of common interoperability standards inhibit seamless integration across sensors, farm management platforms, and GIS applications. The Ag Data Application Programming Toolkit (ADAPT), developed by AgGateway, addresses these challenges by providing an open-source API framework that translates and normalizes data via modular plug-in converters. This approach standardizes data exchange, enhances collaborative workflows, and accelerates the adoption of digital agriculture technologies. Ongoing efforts focus on expanding ADAPT modules for emerging data types and ensuring regulatory compliance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="case-studies" class="slide level2">
<h2>Case Studies</h2>
<p>Comparative analysis of sensor modalities in precision agriculture.</p>
<p>:::</p>
</section>
<section id="crop-health-monitoring-case-study" class="slide level2">
<h2>Crop Health Monitoring Case Study</h2>
<p>Overview of modern remote sensing approaches for high-resolution crop health assessment and management.</p>
<ul>
<li>Platforms: Satellites, manned aircraft &amp; UAVs capturing multispectral, hyperspectral &amp; thermal data<br>
</li>
<li>Smart PA: Real-time sensor feedback with variable-rate applicators for site-specific fertilization<br>
</li>
<li>Operation Tracking: Embedded systems log &amp; geolocate seeding, spraying &amp; fertilizing events<br>
</li>
<li>Harvest-Stage QA: Remote sensing for moisture, nutrient content mapping &amp; batch segregation<br>
</li>
<li>Novel Sensing: In-season &amp; postharvest imaging for stress, disease, pest &amp; weed detection<br>
</li>
<li>Data-Driven: Integrating spatial variability analytics to optimize yield, quality &amp; sustainability</li>
</ul>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb6-1"><a></a>flowchart LR</span>
<span id="cb6-2"><a></a>  S[Satellites/UAVs] --&gt; D[Data Acquisition]</span>
<span id="cb6-3"><a></a>  D --&gt; P[Image Processing &amp; Analysis]</span>
<span id="cb6-4"><a></a>  P --&gt; V[Variable-Rate Application]</span>
<span id="cb6-5"><a></a>  P --&gt; H[Harvest Quality Assessment]</span>
<span id="cb6-6"><a></a>  P --&gt; N[Novel Sensing Techniques]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>In this case study, we examine the evolution of crop health monitoring—from traditional satellite imagery to advanced UAV-based hyperspectral &amp; thermal sensors. Platforms now include low-earth orbit satellites providing daily revisit times, manned aircraft offering fine-scale surveys, and increasingly agile UAVs for sub-meter resolution. Real-time integration with variable-rate applicators enables truly site-specific management: fertilizer and pesticide doses adjust on-the-fly based on NDVI (Normalized Difference Vegetation Index) or thermal stress thresholds. Operation tracking systems log every intervention, associating geospatial coordinates with application parameters to generate an in-field activity map over the season. At harvest, high-resolution remote sensing guides quality assessment by mapping moisture content and nutrient levels across field zones, facilitating precise batch segregation for food safety traceability. Emerging techniques such as fluorescence imaging and machine-learning classification on hyperspectral data quantify nutrient deficiencies, disease symptoms, and weed infestations, delivering actionable insights to optimize both yield and grain quality while minimizing environmental impact.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="soil-moisture-monitoring-case-study" class="slide level2">
<h2>Soil Moisture Monitoring Case Study</h2>
<ul>
<li>Traffic at high moisture contents causes soil compaction and puddling<br>
</li>
<li>Remote and proximal sensors effectively monitor surface moisture but miss upper-horizon conditions<br>
</li>
<li>In-situ subsurface sensors (Cosh et al., 2012) capture moisture dynamics within distinct soil horizons<br>
</li>
<li>Soil horizon: layered zones with unique texture, structure, and hydraulic properties influencing water retention<br>
</li>
<li>Soil-moisture tensiometers measure matric tension to optimize irrigation scheduling and detect drainage issues<br>
</li>
<li>Integration with crop sensors enables predictive water-stress management rather than reactive responses</li>
</ul>
<aside class="notes">
<p>This case study examines a multi-sensor approach to soil moisture monitoring. High moisture content under traffic loads leads to compaction and puddling, which impairs root growth and water infiltration. While remote (e.g., satellite, UAV) and proximal (e.g., gamma density, spectral reflectance) sensors provide surface moisture estimates, they cannot resolve moisture at depth. Cosh et al.&nbsp;(2012) demonstrated that in-situ capacitance and dielectric sensors placed at multiple soil horizons (A, B, C) yield high-resolution vertical moisture profiles. Each soil horizon has distinct porosity and hydraulic conductivity, so targeted sensor placement is critical. Soil-moisture tensiometers measure matric potential to guide irrigation events precisely, reducing over-watering and nutrient leaching. When combined with NDVI or PRI crop sensors, this layered monitoring framework enables predictive stress detection—shifting management from reactive to proactive irrigation scheduling and ultimately improving water-use efficiency and crop yield.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="acquiring-satellite-data" class="slide level2">
<h2>Acquiring Satellite Data</h2>
<ul>
<li>Raster satellite data from WEnR and NEO WCS: NDVI &amp; WDVI biomass maps<br>
</li>
<li>Imagery applied to haulm-killing monitoring and crop vigor assessment<br>
</li>
<li>In-field sensor data ingestion: EM38 soil conductivity, Veris soil mapping, drone-derived NVDE &amp; WDVI<br>
</li>
<li>Automatic API-based delivery directly into the user’s Akkerweb account<br>
</li>
<li>Advisory app integration for storing and reusing third-party recommendations<br>
</li>
<li>Data sharing with granular permissions: time-limited, read-only, or editable</li>
</ul>
<aside class="notes">
<p>Akkerweb’s data acquisition module streamlines the ingestion of both satellite and ground-truth sensor datasets for precision agriculture. Raster satellite data are accessed via OGC-compliant WCS endpoints (WEnR and NEO), delivering high-resolution NDVI and WDVI biomass maps. These spectral indices quantify vegetative vigor, inform nitrogen status, and support tasks such as haulm-killing monitoring by detecting canopy senescence patterns.</p>
<p>In-field sensor ordering integrates proven soil electrical conductivity probes (EM38 by Geonics) and Veris soil mapping platforms. These measure apparent conductivity and infer soil texture, salinity, and moisture variability. Drone deployments from dronewerker.nl capture normalized vegetation density estimates (NVDE) and WDVI at sub-meter resolution, providing flexible, on-demand field imaging. All data streams are ingested via secure APIs directly into the user’s Akkerweb account, ensuring lineage and metadata preservation.</p>
<p>Beyond data capture, Akkerweb supports advisory app integration: third-party agronomic recommendations (e.g., variable rate prescriptions) can be stored, retrieved, and applied within the platform, though they remain hidden from the farmer until validation. Finally, the system’s sharing controls allow users to exchange maps, reports, and raw sensor outputs under customizable permission models—time-limited, read-only, or editable—facilitating collaboration while maintaining data governance.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="advanced-image-processing-techniques" class="slide level2">
<h2>Advanced Image Processing Techniques</h2>
<ul>
<li>Brovey Transform: linear fusion of high-res PAN and low-res MS bands (Zhang et al., 2010)<br>
</li>
<li>Dictionary-Based Atom Extraction: overcomplete sparse dictionary for patch-level detail enhancement<br>
</li>
<li>Wavelet-Based Multi-Resolution Analysis (MRA): decompose PAN into subbands, replace approximations with MS, inverse transform<br>
</li>
<li>MRA Advantages: high spatial-spectral fidelity, reduced distortion, strong SNR (Mather, 2004)<br>
</li>
<li>Trade-Offs: Brovey simplicity vs.&nbsp;spectral distortion; sparse methods control noise but require heavy computation<br>
</li>
<li>Pan-Sharpening: Use of a panchromatic imagery to enhance the spatial resolution of multispectral data.</li>
</ul>
<aside class="notes">
<p>This slide examines state-of-the-art pan-sharpening and data-fusion workflows. The Brovey transform remains widely used for its simple linear combination of PAN and MS data, though it can introduce spectral shifts. Dictionary-based approaches build an overcomplete set of atoms, enabling sparse reconstruction of image patches that recover fine spatial details while suppressing noise. Multi-Resolution Analysis via wavelets decomposes the PAN image into scale-specific subbands, substitutes MS band approximations, and then synthesizes sharpened MS outputs through inverse wavelet transforms. MRA methods generally achieve the best balance of spatial and spectral preservation with minimal artifacts, as documented by Mather (2004). In practice, hybrid pipelines often cascade Brovey or similar linear fusion with sparse or MRA modules to exploit complementary strengths.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="introduction-to-vegetation-indices" class="slide level2">
<h2>Introduction to Vegetation Indices</h2>
<ul>
<li>Several spectral indices (RVI, NDVI, AVI, MTVI) monitor crop growth<br>
</li>
<li>Indices and soil parameters are evaluated for correlation with yield<br>
</li>
<li>Highest-correlating variables combined into a multi-parameter field map<br>
</li>
<li>Geostatistical interpolation and clustering delineate management zones<br>
</li>
<li>In a low-variability field, two management zones were identified<br>
</li>
<li>Validation: 83 of 99 pixels matched high/low yield zones</li>
</ul>
<aside class="notes">
<p>This slide introduces the core remote-sensing vegetation indices used in precision agriculture: ratio vegetation index (RVI), normalized difference vegetation index (NDVI), agricultural vegetation index (AVI), and multitemporal vegetation index (MTVI). Each index leverages canopy reflectance in specific spectral bands to estimate crop biophysical parameters such as leaf area index and chlorophyll content. We then assess the statistical correlation between these spectral indices and traditional soil analysis parameters against measured yield data. Variables showing the highest correlation are integrated into a composite, multi-parameter map of the field. To partition the field into management zones, we apply geostatistical interpolation techniques (e.g., kriging) followed by clustering algorithms. In our study of a low-variability field, this process yielded two distinct zones. Validation against actual yield measurements confirmed that 83 out of 99 pixels were accurately classified into their respective high- or low-yield zones, demonstrating a strong concordance between the combined soil-spectral zoning map and empirical yield data. This evidence supports the adoption of integrated soil and vegetation index strategies for precision crop management.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="normalized-difference-vegetation-index-ndvi" class="slide level2">
<h2>Normalized Difference Vegetation Index (NDVI)</h2>
<ul>
<li>Computes vegetation vigor using (NIR – Red) / (NIR + Red)<br>
</li>
<li>Typical NDVI range: 0 (bare soil) to 1 (dense green canopy)<br>
</li>
<li>Strongly correlates with chlorophyll content, leaf area index, biomass<br>
</li>
<li>Pixel-level mapping reveals intra-field variability and stress zones<br>
</li>
<li>Basis for delineating management zones (MZs) for variable-rate input<br>
</li>
<li>Enables precision decisions: targeted fertilization &amp; irrigation</li>
</ul>
<aside class="notes">
<p>NDVI is a cornerstone spectral index in precision agriculture, transforming red and near-infrared reflectance into quantitative indicators of plant health. Empirical studies demonstrate its strong correlation with canopy attributes such as chlorophyll concentration, leaf area index (LAI) and aboveground biomass. By assigning each image pixel an NDVI value between 0 and 1, agronomists can generate high-resolution vigor maps that distinguish soil background variability from true vegetative stress. These maps facilitate the delineation of management zones (MZs) optimized for variable-rate fertilization, irrigation scheduling and other site-specific interventions. Research shows that NDVI-guided management can increase input use efficiency by 10–30 % and improve yield uniformity across diverse cropping systems. When integrated with GPS-guided application systems, NDVI data drive actionable prescriptions that enhance sustainability and profitability.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="enhanced-vegetation-index-evi" class="slide level2">
<h2>Enhanced Vegetation Index (EVI)</h2>
<ul>
<li>Builds on NDVI by integrating blue-band reflectance for atmospheric and canopy background correction<br>
</li>
<li>Formula: EVI = G * (NIR - Red) / (NIR + C1 * Red - C2 * Blue + L)<br>
</li>
<li>Reduces sensitivity to soil brightness, aerosol scattering, and leaf geometry variations<br>
</li>
<li>Optimized for broad-scale monitoring under dense canopies and high aerosol conditions<br>
</li>
<li>Provides improved dynamic range in high-biomass regions compared to NDVI<br>
</li>
<li>Widely applied in satellite platforms (MODIS, Sentinel-3) for ecosystem and crop mapping</li>
</ul>
<aside class="notes">
<p>The Enhanced Vegetation Index (EVI) was developed by NASA to overcome several limitations of NDVI, particularly in high-biomass and aerosol-laden environments. By incorporating a blue-band correction term, EVI compensates for atmospheric scattering and canopy background reflectance. The standard EVI algorithm uses gain (G), soil adjustment (L), and blue (C2) and red (C1) coefficients tuned from MODIS data:</p>
<p>EVI = 2.5 * (NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1)</p>
<p>Research shows that EVI exhibits a wider dynamic range in dense canopies, reducing saturation effects common in NDVI. It is extensively validated for monitoring vegetation phenology, biomass estimation, and stress detection on global to regional scales. Sentinel-3 and MODIS products routinely provide EVI layers, enabling integration with other spectral indices and time-series analysis for robust precision agriculture applications.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="normalized-difference-red-edge-index-ndre" class="slide level2">
<h2>Normalized Difference Red Edge Index (NDRE)</h2>
<ul>
<li>Uses red-edge (~730 nm) and NIR (~780 nm) reflectance bands<br>
</li>
<li>Higher sensitivity than NDVI in dense or late-season canopies<br>
</li>
<li>Less saturation under high biomass, improves chlorophyll estimation<br>
</li>
<li>Calculation: NDRE = (NIR – RedEdge) / (NIR + RedEdge)<br>
</li>
<li>Example: (0.323 – 0.209) / (0.323 + 0.209) ≈ 0.214<br>
</li>
<li>Enables NDRE-driven variable-rate nitrogen applications</li>
</ul>
<aside class="notes">
<p>The Normalized Difference Red Edge (NDRE) index leverages the red-edge spectral band centered near 730 nm together with a near-infrared (NIR) band around 780 nm. Research shows that chlorophyll absorption features at the red-edge provide a more linear response under high biomass conditions compared to the classical NDVI, which often saturates in closed canopies. NDRE is calculated by taking the difference between NIR and red-edge reflectance and normalizing by their sum—this simple ratio reduces the impact of illumination differences and sensor drift.</p>
<p>In Problem 9.10, canopy sensor readings yielded a red-edge reflectance of 0.209 and an NIR reflectance of 0.323. Substituting into the NDRE formula produces a value of approximately 0.214, indicating moderate chlorophyll density. Peer-reviewed studies (e.g., Gitelson et al.&nbsp;2016) validate that NDRE correlates strongly (R² &gt; 0.85) with leaf chlorophyll content in corn and wheat during V6–V12 growth stages.</p>
<p>Figure 9.15 demonstrates a sidedress nitrogen-rate map generated by an NDRE-based algorithm, highlighting spatial nitrogen variability within the field. By integrating NDRE with georeferenced application equipment, farmers can apply nitrogen at variable rates, improving nitrogen use efficiency by 10–15 % and reducing environmental leaching risks. When used alongside NDVI, NDRE enhances monitoring across the entire season—from early growth through peak biomass and senescence.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="normalized-difference-water-index-ndwi" class="slide level2">
<h2>Normalized Difference Water Index (NDWI)</h2>
<ul>
<li>NDWI–Hyperion defined as (R1070 – R1200) / (R1070 + R1200)<br>
</li>
<li>Sensitive to water absorption peak near 1200 nm (Ustin et al.&nbsp;2002)<br>
</li>
<li>Demonstrates robust correlation with LWC (R² ≈ 0.42–0.50 across conditions)<br>
</li>
<li>Outperforms single-band indices by normalizing bidirectional reflectance effects<br>
</li>
<li>Comparable to Vegetation Dry Index and NIR index in predictive accuracy<br>
</li>
<li>Readily implementable on UAV and satellite multispectral sensors</li>
</ul>
<aside class="notes">
<p>The Normalized Difference Water Index (NDWI) exploits the differential absorption properties of leaf water around 1200 nm to estimate leaf water content. Originally introduced by Ustin et al.&nbsp;(2002) for Hyperion imagery, the index normalizes reflectance at a water absorption band (1200 nm) against a nearby non-absorbing band (1070 nm), mitigating illumination and structural noise. Empirical studies report R² values between 0.42 and 0.50 when correlating NDWI with laboratory-measured LWC, outperforming single-band ratios. Comparative analyses show NDWI’s performance is on par with the Vegetation Dry Index [(R970–R900)/(R970+R900)] and the NIR index [(R850–R1650)/(R850+R1650)]. Its simplicity and spectral requirements make NDWI highly adaptable to both UAV-mounted multispectral cameras and satellite platforms, facilitating scalable water stress monitoring across diverse agroecosystems.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="soil-adjusted-vegetation-index-savi" class="slide level2">
<h2>Soil-Adjusted Vegetation Index (SAVI)</h2>
<ul>
<li>Incorporates soil-adjustment factor L to reduce background soil signal<br>
</li>
<li>Defined as SAVI = (NIR – RED) / (NIR + RED + L) × (1 + L)<br>
</li>
<li>Typical L = 0.5 for intermediate vegetation cover, adjustable based on canopy density<br>
</li>
<li>Enhances sensitivity to green biomass in early growth stages over bare soils<br>
</li>
<li>Demonstrated stronger correlations with leaf area index (LAI) and chlorophyll content vs.&nbsp;NDVI<br>
</li>
<li>Integral in precision nitrogen management by refining spatial N status estimates</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb7-1"><a></a>flowchart LR</span>
<span id="cb7-2"><a></a>  A[NIR Reflectance] --&gt; D[Num: NIR - RED]</span>
<span id="cb7-3"><a></a>  B[Red Reflectance] --&gt; D</span>
<span id="cb7-4"><a></a>  A --&gt; E[Den: NIR + RED + L]</span>
<span id="cb7-5"><a></a>  B --&gt; E</span>
<span id="cb7-6"><a></a>  C[Soil Factor L] --&gt; E</span>
<span id="cb7-7"><a></a>  D --&gt; F[/ Division /]</span>
<span id="cb7-8"><a></a>  E --&gt; F</span>
<span id="cb7-9"><a></a>  F --&gt; G[Multiply by (1+L)]</span>
<span id="cb7-10"><a></a>  G --&gt; H[SAVI]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>The Soil-Adjusted Vegetation Index (SAVI) was introduced to address the limitations of NDVI in areas with sparse vegetation, where soil brightness can dominate the spectral signal. By incorporating the soil-adjustment factor L, SAVI minimizes soil background effects, improving the dynamic range of VI values under low canopy cover.</p>
<p>Research shows that selecting an appropriate L (commonly 0.5 for moderate cover) optimizes sensitivity across growth stages. Comparative studies have demonstrated that SAVI exhibits higher correlation coefficients with ground-measured LAI, biomass, and chlorophyll content than NDVI, particularly during early vegetative phases.</p>
<p>In precision agriculture, SAVI-based maps enable more accurate nitrogen status assessments and variable-rate fertilization prescriptions, leading to optimized nutrient use efficiency and yield gains.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="translating-index-values-to-insights" class="slide level2">
<h2>Translating Index Values to Insights</h2>
<ul>
<li>ISU formula: (n₁ + n₂) / n₃ quantifies seeding uniformity<br>
</li>
<li>n₁: clusters (&lt; 25 % of target distance); n₂: gaps (&gt; 200 %)<br>
</li>
<li>n₃: preferred range gaps (± 20 % of target)<br>
</li>
<li>Adjustable threshold parameters align ISU with crop-specific criteria<br>
</li>
<li>Lower ISU values indicate higher spacing conformity<br>
</li>
<li>Supports seeder performance benchmarking and gap-filling strategies</li>
</ul>
<aside class="notes">
<p>The Interplant Space Index (ISU) provides a quantitative measure of seeding uniformity by relating outlier spacings to those within an agronomically optimal range. In this slide, we emphasize that n₁ and n₂ identify extreme spacings – clusters and large gaps – while n₃ captures the count of acceptable interplant distances. Practitioners can adjust the ± 20 % preferred range and outlier thresholds to reflect specific crop requirements or planting equipment characteristics. A declining ISU signifies increasingly consistent spacing, which has been corroborated by field trials described in Smith et al.&nbsp;(2018) and Gonzalez &amp; Zhang (2020). ISU’s sensitivity to both excessive clustering and gapping makes it superior for evaluating and comparing planter performance across different soil conditions and machine settings. Moreover, integrating ISU into yield-monitoring algorithms enables targeted gap-filling interventions, reducing yield variability and enhancing overall productivity, as supported by recent big-data studies in precision agriculture.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="calculating-ndvi-in-practice" class="slide level2">
<h2>Calculating NDVI in Practice</h2>
<ul>
<li>NDVI reliably measures crop vigor; soil background effects are minimal<br>
</li>
<li>Wheat NDVI values range ~0.4 at tillering to ~0.95 at full canopy<br>
</li>
<li>Satellite imagery: large-area coverage, rapid revisit, cloud-cover limitations<br>
</li>
<li>Drone/UAS imaging: flexible scheduling under clouds, high resolution, limited footprint<br>
</li>
<li>Key applications: nutrient deficiency, insect damage, water stress detection<br>
</li>
<li>Time-series NDVI for seasonal and multi-year crop monitoring</li>
</ul>
<div class="cell" data-reveal="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class=""><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  S(Satellite Imagery) --&gt; P(Data Processing)
 
  P --&gt; C[NDVI Calculation]
  C --&gt; Z[Stress Detection &amp; Zoning]
  C --&gt; T[Time-Series Monitoring]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<aside class="notes">
<p>Normalized Difference Vegetation Index (NDVI) is computed as (NIR – Red)/(NIR + Red). In practice, the minimal influence of bare soil reflectance ensures NDVI consistently tracks leaf area and chlorophyll activity. For wheat, NDVI values begin near 0.4 during early tillering and can exceed 0.9 under dense, healthy canopy.</p>
<p>Satellite platforms (e.g., Sentinel-2, Landsat 8) enable rapid mapping of large fields but are constrained by cloud cover and coarse spatial resolution (10–30 m). Unmanned aerial systems equipped with single-band NIR sensors provide sub-meter resolution and on-demand flights, ideal for localized stress detection. Practitioners use NDVI maps to delineate management zones for variable-rate fertilizer, identify hotspots of nutrient deficiency or insect damage, and optimize irrigation scheduling. Time-series analysis of NDVI further supports trend detection over seasons and years, allowing for yield forecasting and long-term soil health assessment.</p>
<p>Transition to next topic: Next, we will explore hyperspectral vegetation indices and their advantages over NDVI.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="interpreting-index-results" class="slide level2">
<h2>Interpreting Index Results</h2>
<ul>
<li>Definition and formula of I_SU: (n1 + n2) / n3<br>
</li>
<li>n1 = count of distances &lt; 0.25× target (clusters); n2 = count &gt; 2× target (gaps)<br>
</li>
<li>n3 = count of distances within ± 20 % of target spacing (preferred range)<br>
</li>
<li>Higher I_SU indicates poorer spatial uniformity with more clusters and gaps<br>
</li>
<li>Univariate Moran’s I: 0 = no spatial autocorrelation; ± 1 = strong autocorrelation<br>
</li>
<li>Bivariate Moran’s I evaluates spatial correlation between two variables</li>
</ul>
<aside class="notes">
<p>The Interplant Space Index (I_SU) is a robust metric for quantifying seeder performance by capturing extremes in interplant distances relative to an agronomic target spacing. In practice, n1 tallies measurements indicating overly dense planting (clusters), while n2 counts those representing excessive gaps—both detrimental to uniform crop establishment. The denominator n3 encompasses counts within ± 20 % of the target, delineating a preferred operational window. Adjusting these thresholds allows practitioners to calibrate sensitivity based on crop type or field heterogeneity.</p>
<p>A higher I_SU value corresponds to a greater prevalence of spacing anomalies, signifying suboptimal seed distribution. This index effectively discriminates between planters, guiding equipment tuning and agronomic decisions. Complementarily, spatial autocorrelation metrics such as Moran’s I contextualize these findings: a univariate Moran’s I near zero suggests random spatial patterns, whereas values approaching ± 1 denote strong clustering or dispersion of spacing deviations. Bivariate Moran’s I extends this analysis by linking interplant spacing with secondary variables (e.g., soil moisture, yield), facilitating integrated assessments of spatial dependence in crop performance. Incorporating I_SU alongside Moran’s I supports data-driven optimization of precision planting strategies.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="google-earth-engine" class="slide level2">
<h2>Google Earth Engine</h2>
<video id="video_shortcode_videojs_video1" width="85%" height="85%" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://developers.google.com/earth-engine/datasets/tags/agriculture"></video>
<p><a href="https://developers.google.com/earth-engine/datasets/tags/agriculture">GEE</a> is THE platform/IDE for Remote Sensing Analysis in the cloud.</p>
</section>
<section id="integrating-results-into-farm-management" class="slide level2">
<h2>Integrating Results into Farm Management</h2>
<ul>
<li>Centralized integration of soil, sensor, and environmental data<br>
</li>
<li>Predictive modeling for crop growth, water dynamics, and disease risk<br>
</li>
<li>Farmmaps UI visualizes field-level and intra-field variability<br>
</li>
<li>Model-derived prescriptions for fertilizers, irrigation, and crop protection<br>
</li>
<li>Seamless workflow alignment with farmer management schedules<br>
</li>
<li>Continuous feedback loop refines models based on field outcomes</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb8-1"><a></a>graph LR</span>
<span id="cb8-2"><a></a>  A[Soil &amp; Sensor Data] --&gt; D(Farmmaps Platform)</span>
<span id="cb8-3"><a></a>  B[Weather &amp; Environmental Data] --&gt; D</span>
<span id="cb8-4"><a></a>  C[Yield &amp; Imaging Data] --&gt; D</span>
<span id="cb8-5"><a></a>  D --&gt; E[Variability Visualization]</span>
<span id="cb8-6"><a></a>  D --&gt; F[Prescriptive Recommendations]</span>
<span id="cb8-7"><a></a>  E --&gt; G[Farm Management Actions]</span>
<span id="cb8-8"><a></a>  F --&gt; G</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>This slide details how Dacom’s Farm Intelligence platform operationalizes precision-agriculture insights into tangible farm actions. The system ingests heterogeneous data streams—soil moisture sensors, weather stations, yield monitors, and remote imagery—into a unified architecture. Core agronomic models simulate potato growth stages, soil-water balance, late-blight infection risk, and nematode population dynamics. Through the Farmmaps interface, users visualize spatial and temporal variability at both field and sub-field scales. Model-derived prescriptions generate localized recommendations for nutrient dosing, irrigation scheduling, and crop protection measures. By embedding these outputs into the farmer’s existing workflow, the platform ensures adoption and minimizes decision fatigue. Finally, outcome data are fed back to recalibrate model parameters, creating a closed-loop system that continually improves decision accuracy.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="integrating-satellite-data-with-ground-sensors" class="slide level2">
<h2>Integrating Satellite Data with Ground Sensors</h2>
<ul>
<li>Fusion of geophysical sensors and satellite imagery for comprehensive soil variability mapping<br>
</li>
<li>Combining high-spatial-resolution multispectral data with high-temporal-resolution satellite revisits<br>
</li>
<li>Co-located on-the-go soil probes and stationary EMI/GPR sensors for calibration and validation<br>
</li>
<li>GNSS-enabled yield monitors and crop/soil sensors supplying real-time geo-referenced data<br>
</li>
<li>Spatio-temporal data fusion via kriging, machine learning, and data assimilation workflows<br>
</li>
<li>Multi-layered soil profile characterization: structure, moisture, nutrient distribution</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb9-1"><a></a>flowchart LR</span>
<span id="cb9-2"><a></a>  SAT[Satellite Imagery]</span>
<span id="cb9-3"><a></a>  GND[Ground Sensors]</span>
<span id="cb9-4"><a></a>  FUS[Data Fusion Engine]</span>
<span id="cb9-5"><a></a>  MAP[High-Resolution Soil Maps]</span>
<span id="cb9-6"><a></a></span>
<span id="cb9-7"><a></a>  SAT --&gt; FUS</span>
<span id="cb9-8"><a></a>  GND --&gt; FUS</span>
<span id="cb9-9"><a></a>  FUS --&gt; MAP</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>Precision agriculture leverages complementary data sources to map soil variability more accurately. High-spatial-resolution multi- and hyperspectral satellite imagery capture fine detail in surface reflectance, while frequent revisit times provide temporal dynamics of crop and soil conditions. Geophysical sensors—EMI, electrical resistivity, ground-penetrating radar—deliver subsurface soil property measurements that are vital for understanding soil structure and moisture distribution.</p>
<p>On-the-go soil probes allow continuous profiling across multiple depths during field operations, providing calibration points for remote sensing estimates. Static ground sensors (e.g., fluorimeters, gamma detectors) further validate satellite-derived variables and help quantify uncertainty. Modern systems integrate yield monitors with crop and soil property sensors, all geo-referenced by high-accuracy GNSS receivers, enabling real-time spatial correlation of soil and yield data.</p>
<p>Fusing these datasets requires advanced geostatistical and machine learning workflows. Techniques such as spatio-temporal kriging, data assimilation, and ensemble learning merge high-resolution spatial snapshots with time-series data, producing dynamic soil maps. The resulting multi-layered soil profiles inform precision management decisions—optimizing irrigation, fertilization, and tillage based on site-specific soil conditions.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="data-management-and-big-data-considerations" class="slide level2">
<h2>Data Management and Big Data Considerations</h2>
<ul>
<li>Economic assessment tools evaluate capital investments, land rental, practice changes, and labor costs<br>
</li>
<li>Big data in agriculture features high volume, variety, and complexity requiring specialized architectures<br>
</li>
<li>Multi-year yield and cost analyses pinpoint unprofitable field zones for targeted interventions<br>
</li>
<li>Cloud-based archiving and automated backups of raw yield-monitor outputs ensure long-term data integrity<br>
</li>
<li>Clearly labeled folder structures by year and data type facilitate retrospective analyses<br>
</li>
<li>Integration with decision support systems drives timely, data-driven management decisions</li>
</ul>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource mermaid number-lines code-with-copy"><code class="sourceCode"><span id="cb10-1"><a></a>flowchart LR</span>
<span id="cb10-2"><a></a>  A[Data Sources: Sensors, Drones, Satellites] --&gt; B[Cloud Ingestion &amp; Storage]</span>
<span id="cb10-3"><a></a>  B --&gt; C[Data Archival &amp; Backup]</span>
<span id="cb10-4"><a></a>  C --&gt; D[ETL &amp; Preprocessing]</span>
<span id="cb10-5"><a></a>  D --&gt; E[Analytics &amp; Economic Assessment]</span>
<span id="cb10-6"><a></a>  E --&gt; F[Decision Support &amp; Management Actions]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<aside class="notes">
<p>Effective data management underpins precision agriculture’s ability to leverage big data. Economic assessment solutions go beyond traditional FMIS by focusing on ROI for machinery acquisitions, land leases, split nitrogen applications, cover crops, and crop rotations. The influx of data from IoT sensors, drones, and mobile devices creates challenges in volume, variety, and velocity (Fausti &amp; Wang, 2017). To address this, robust cloud architectures and ETL pipelines are deployed for ingestion, storage, and preprocessing. Multi-year analyses of yield and cost metrics enable the identification of loss-making zones, guiding targeted agronomic practices. Best practices include automated cloud backups of raw yield-monitor outputs and a standardized folder taxonomy by year and data type. This structured approach preserves historical data integrity, supports advanced analytics, and fuels decision support tools for optimized farm management.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="emerging-technologies-in-agri-remote-sensing" class="slide level2">
<h2>Emerging Technologies in Agri Remote Sensing</h2>
<ul>
<li>Integration of &gt; 175 imagery layers with weather, yield, planting/spraying records, harvest details, and field notes<br>
</li>
<li>Advances in sUAS enabling low-altitude, high-resolution multispectral and thermal data capture<br>
</li>
<li>Electromagnetic spectrum coverage spans UV, visible, NIR, thermal IR, and microwave bands for diverse crop and soil insights<br>
</li>
<li>Platform diversity: satellites, manned aircraft, sUAS, and vehicle-mounted units offering spatial resolutions from sub-meter to 10 m+<br>
</li>
<li>Nonimaging spectroradiometers record discrete spectral signatures; imaging sensors (multispectral &amp; hyperspectral cameras) capture spatially resolved spectra<br>
</li>
<li>Sensor fusion workflows support variable temporal revisits, enhancing real-time crop monitoring and prescription accuracy</li>
</ul>
<aside class="notes">
<p>Remote sensing has been integral to precision agriculture since the 1980s, measuring reflected and emitted electromagnetic radiation across UV, visible, NIR, thermal IR, and microwave bands (Mulla 2013). Modern platforms range from high-orbit satellites to low-altitude sUAS and ground vehicles, enabling tailored spatial resolutions for specific management zones. Today’s systems integrate more than 175 imagery layers with ancillary datasets—weather, yield potential, planting, spraying records, harvest details, and field notes—to drive precise decision-making. Nonimaging sensors like spectroradiometers capture point-based spectral signatures at discrete wavelengths, while imaging sensors such as multispectral and hyperspectral cameras provide spatially contiguous spectral data for advanced classification and stress detection. sUAS have democratized access to sub-meter resolution imagery, allowing frequent, on-demand surveys. This slide outlines the technological landscape, highlighting the sensor types, platforms, and spectral domains essential for next-generation agronomic insights.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="future-directions-in-satellite-data-for-farming" class="slide level2">
<h2>Future Directions in Satellite Data for Farming</h2>
<ul>
<li>Integration of high-frequency small-satellite constellations for daily/sub-daily revisit<br>
</li>
<li>Advancements in SAR-based all-weather monitoring for continuous stress detection<br>
</li>
<li>Fusion of multispectral, thermal, and radar data to distinguish water vs.&nbsp;nutrient limitations<br>
</li>
<li>Pixel-level time-series machine learning models for early pest and disease outbreak prediction<br>
</li>
<li>Development of calibrated spectral indices for direct quantification of nutrient deficiencies<br>
</li>
<li>Coupling satellite-derived evapotranspiration and soil moisture models with AI-driven irrigation scheduling</li>
</ul>
<aside class="notes">
<p>Ongoing research in precision agriculture is focused on overcoming the limitations of traditional satellite imagery, notably infrequent revisit times and cloud cover. Small-satellite constellations (e.g., PlanetScope) can now deliver daily or sub-daily data streams that enable finer temporal resolution. Synthetic Aperture Radar (SAR) platforms such as Sentinel-1 provide cloud-penetrating capability, allowing continuous monitoring of crop structure and biomass. Data fusion across multispectral, thermal, and radar sensors helps disaggregate signals caused by water stress versus nutrient deficiency, a key challenge identified by Mulla (2013). Machine-learning approaches applied to pixel-level time series can detect subtle phenological shifts indicative of pest or disease onset before visual symptoms emerge. Moreover, researchers are developing spectral indices calibrated to nutrient concentration, eliminating the need for time-consuming in-field reference treatments (Clay et al., 2014; Lee et al., 2014). Finally, integrating satellite-based evapotranspiration estimates with AI-driven irrigation models promises to optimize water use efficiency in real time.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="qa-and-thank-you" class="slide level2">
<h2>Q&amp;A and Thank You</h2>
<ul>
<li>Questions on applying QGIS in precision agriculture workflows?<br>
</li>
<li>Discussion on integrating drone and sensor data with spatial analysis<br>
</li>
<li>Queries about customizing plugins for farm management tools<br>
</li>
<li>Feedback on the open-access QGIS learning resource structure<br>
</li>
<li>Collaboration opportunities for data sharing and community development<br>
</li>
<li>Next steps: exploring tutorials, forums, and contributing back</li>
</ul>
<aside class="notes">
<p>Thank you for your attention. This final slide opens the floor for questions on how QGIS can be leveraged in precision agriculture, including use cases integrating UAV imagery and field sensor networks with spatial analytics. You may ask about developing or adapting plugins for specific agronomic tasks—such as variable-rate application or soil health monitoring—drawing from the open-access QGIS resource funded by the DLA.</p>
<p>We encourage feedback on the structure of the GentleGIS learning modules, the flow from map creation to advanced customization, and the incorporation of community-driven tutorials. Collaboration is central to the Ubuntu philosophy underpinning this initiative: please share insights on data sharing protocols, metadata standards, and joint development workflows. Finally, we’ll discuss next steps: engaging with QGIS forums, contributing to the GitHub repository, and expanding the resource through your domain expertise.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1280,

        height: 720,

        // Factor of the display size that should remain empty around the content
        margin: 5.0e-2,

        // Bounds for smallest/largest possible scale to apply to content
        minScale: 0.5,

        maxScale: 1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    <script>videojs(video_shortcode_videojs_video1);</script>
    

</body></html>